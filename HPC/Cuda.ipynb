{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":150545,"sourceType":"datasetVersion","datasetId":70909}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!nvidia-smi","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-06T16:27:42.862265Z","iopub.execute_input":"2025-05-06T16:27:42.862606Z","iopub.status.idle":"2025-05-06T16:27:43.034133Z","shell.execute_reply.started":"2025-05-06T16:27:42.862572Z","shell.execute_reply":"2025-05-06T16:27:43.033111Z"}},"outputs":[{"name":"stdout","text":"Tue May  6 16:27:42 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   34C    P0             26W /  250W |       0MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"%%writefile vector_add.cu\n#include <iostream>\n#include <vector>\n#include <random>\n#include <cuda_runtime.h>\n#include <omp.h>\n#include <iomanip>\n\nusing namespace std;\n\n// CUDA error checking macro\n#define CUDA_CHECK(call) \\\n    do { \\\n        cudaError_t err = call; \\\n        if (err != cudaSuccess) { \\\n            cerr << \"CUDA Error: \" << cudaGetErrorString(err) << \" at line \" << __LINE__ << endl; \\\n            exit(EXIT_FAILURE); \\\n        } \\\n    } while(0)\n\n// Kernel for vector addition\n__global__ void vectorAddKernel(float* A, float* B, float* C, int N) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        C[idx] = A[idx] + B[idx];\n    }\n}\n\n// Sequential vector addition for verification\nvoid sequentialVectorAdd(const vector<float>& A, const vector<float>& B, vector<float>& C, int N) {\n    for (int i = 0; i < N; ++i) {\n        C[i] = A[i] + B[i];\n    }\n}\n\n// Function to generate random vector\nvector<float> generateRandomVector(int N) {\n    vector<float> vec(N);\n    random_device rd;\n    mt19937 gen(rd());\n    uniform_real_distribution<float> dis(0.0f, 10.0f);\n    for (int i = 0; i < N; ++i) {\n        vec[i] = dis(gen);\n    }\n    return vec;\n}\n\nint main() {\n    int N;\n    char choice;\n\n    // Input vector length - Modified for automated testing\n    N = 50000000;  // Using a large value for better timing comparison\n    choice = 'y'; // Always use random vectors in Kaggle\n\n    vector<float> A, B, C(N), C_seq(N);\n    if (choice == 'y' || choice == 'Y') {\n        A = generateRandomVector(N);\n        B = generateRandomVector(N);\n    } else {\n        A.resize(N);\n        B.resize(N);\n        cout << \"Enter \" << N << \" elements for vector A:\\n\";\n        for (int i = 0; i < N; ++i) cin >> A[i];\n        cout << \"Enter \" << N << \" elements for vector B:\\n\";\n        for (int i = 0; i < N; ++i) cin >> B[i];\n    }\n\n    // Allocate device memory\n    float *d_A, *d_B, *d_C;\n    CUDA_CHECK(cudaMalloc(&d_A, N * sizeof(float)));\n    CUDA_CHECK(cudaMalloc(&d_B, N * sizeof(float)));\n    CUDA_CHECK(cudaMalloc(&d_C, N * sizeof(float)));\n\n    // Copy data to device\n    CUDA_CHECK(cudaMemcpy(d_A, A.data(), N * sizeof(float), cudaMemcpyHostToDevice));\n    CUDA_CHECK(cudaMemcpy(d_B, B.data(), N * sizeof(float), cudaMemcpyHostToDevice));\n\n    // Set up kernel launch parameters\n    int threadsPerBlock = 256;\n    int blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock;\n\n    // Create CUDA events for timing\n    cudaEvent_t start, stop;\n    CUDA_CHECK(cudaEventCreate(&start));\n    CUDA_CHECK(cudaEventCreate(&stop));\n\n    // Launch kernel and measure time\n    CUDA_CHECK(cudaEventRecord(start));\n    vectorAddKernel<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, N);\n    CUDA_CHECK(cudaEventRecord(stop));\n    CUDA_CHECK(cudaEventSynchronize(stop));\n\n    float cuda_time_ms;\n    CUDA_CHECK(cudaEventElapsedTime(&cuda_time_ms, start, stop));\n\n    // Copy result back to host\n    CUDA_CHECK(cudaMemcpy(C.data(), d_C, N * sizeof(float), cudaMemcpyDeviceToHost));\n\n    // Sequential vector addition for timing\n    double seq_start = omp_get_wtime();\n    sequentialVectorAdd(A, B, C_seq, N);\n    double seq_end = omp_get_wtime();\n    double seq_time = seq_end - seq_start;\n\n    // Output result vector (trimmed if large)\n    cout << \"\\nResult Vector C (first 5 elements):\\n\";\n    for (int i = 0; i < min(5, N); ++i) {\n        cout << C[i] << \" \";\n    }\n    if (N > 5) cout << \"...\";\n    cout << endl;\n\n    // Output execution times and stats\n    cout << \"\\nExecution Times:\\n\";\n    cout << \"CUDA Vector Addition: \" << fixed << setprecision(6) << cuda_time_ms / 1000.0 << \" seconds\\n\";\n    cout << \"Sequential Vector Addition: \" << seq_time << \" seconds\\n\";\n    cout << \"Speedup (Sequential / CUDA): \" << seq_time / (cuda_time_ms / 1000.0) << \"x\\n\";\n\n    // Free device memory\n    CUDA_CHECK(cudaFree(d_A));\n    CUDA_CHECK(cudaFree(d_B));\n    CUDA_CHECK(cudaFree(d_C));\n    CUDA_CHECK(cudaEventDestroy(start));\n    CUDA_CHECK(cudaEventDestroy(stop));\n\n    return 0;\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T16:33:13.071995Z","iopub.execute_input":"2025-05-06T16:33:13.072292Z","iopub.status.idle":"2025-05-06T16:33:13.081439Z","shell.execute_reply.started":"2025-05-06T16:33:13.072271Z","shell.execute_reply":"2025-05-06T16:33:13.080433Z"}},"outputs":[{"name":"stdout","text":"Overwriting vector_add.cu\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!nvcc vector_add.cu -o vector_add -Xcompiler -fopenmp","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T16:33:17.461028Z","iopub.execute_input":"2025-05-06T16:33:17.461569Z","iopub.status.idle":"2025-05-06T16:33:20.573691Z","shell.execute_reply.started":"2025-05-06T16:33:17.461541Z","shell.execute_reply":"2025-05-06T16:33:20.572540Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"!./vector_add","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T16:33:22.425952Z","iopub.execute_input":"2025-05-06T16:33:22.426283Z","iopub.status.idle":"2025-05-06T16:33:30.030296Z","shell.execute_reply.started":"2025-05-06T16:33:22.426248Z","shell.execute_reply":"2025-05-06T16:33:30.029288Z"}},"outputs":[{"name":"stdout","text":"\nResult Vector C (first 5 elements):\n7.60013 4.33247 4.89701 11.7373 6.96187 ...\n\nExecution Times:\nCUDA Vector Addition: 0.001393 seconds\nSequential Vector Addition: 0.452184 seconds\nSpeedup (Sequential / CUDA): 324.725415x\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"%%writefile matrix_mul.cu\n#include <iostream>\n#include <vector>\n#include <random>\n#include <cuda_runtime.h>\n#include <iomanip> // For setprecision\n#include <omp.h> // For omp_get_wtime\n\nusing namespace std;\n\n// CUDA error checking macro\n#define CUDA_CHECK(call) \\\n    do { \\\n        cudaError_t err = call; \\\n        if (err != cudaSuccess) { \\\n            cerr << \"CUDA Error: \" << cudaGetErrorString(err) << \" at line \" << __LINE__ << endl; \\\n            exit(EXIT_FAILURE); \\\n        } \\\n    } while(0)\n\n// Kernel for tiled matrix multiplication\n__global__ void matrixMulKernel(float* A, float* B, float* C, int M, int N, int K) {\n    const int TILE_SIZE = 16;\n    __shared__ float As[16][16];\n    __shared__ float Bs[16][16];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float sum = 0.0f;\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        // Load tiles into shared memory\n        if (row < M && t * TILE_SIZE + threadIdx.x < K)\n            As[threadIdx.y][threadIdx.x] = A[row * K + t * TILE_SIZE + threadIdx.x];\n        else\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n\n        if (col < N && t * TILE_SIZE + threadIdx.y < K)\n            Bs[threadIdx.y][threadIdx.x] = B[(t * TILE_SIZE + threadIdx.y) * N + col];\n        else\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n\n        __syncthreads();\n\n        // Compute partial sum\n        for (int i = 0; i < TILE_SIZE; ++i)\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N)\n        C[row * N + col] = sum;\n}\n\n// Sequential matrix multiplication for verification\nvoid sequentialMatrixMul(const vector<float>& A, const vector<float>& B, vector<float>& C, int M, int N, int K) {\n    for (int i = 0; i < M; ++i) {\n        for (int j = 0; j < N; ++j) {\n            float sum = 0.0f;\n            for (int k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}\n\n// Function to generate random matrix\nvector<float> generateRandomMatrix(int rows, int cols) {\n    vector<float> mat(rows * cols);\n    random_device rd;\n    mt19937 gen(rd());\n    uniform_real_distribution<float> dis(0.0f, 10.0f);\n    for (int i = 0; i < rows * cols; ++i) {\n        mat[i] = dis(gen);\n    }\n    return mat;\n}\n\nint main() {\n    int M = 1024;  // Matrix A: 1024 rows\n    int K = 1024;  // Matrix A: 1024 columns, Matrix B: 1024 rows\n    int N = 1024;  // Matrix B: 1024 columns\n    \n    cout << \"Using hardcoded matrix dimensions: A(\" << M << \"x\" << K << \") * B(\" << K << \"x\" << N << \") = C(\" << M << \"x\" << N << \")\" << endl;\n    \n    // Always use random matrices\n    cout << \"Generating random matrices...\" << endl;\n    vector<float> A = generateRandomMatrix(M, K);\n    vector<float> B = generateRandomMatrix(K, N);\n    vector<float> C(M * N), C_seq(M * N);\n\n    // Allocate device memory\n    float *d_A, *d_B, *d_C;\n    CUDA_CHECK(cudaMalloc(&d_A, M * K * sizeof(float)));\n    CUDA_CHECK(cudaMalloc(&d_B, K * N * sizeof(float)));\n    CUDA_CHECK(cudaMalloc(&d_C, M * N * sizeof(float)));\n\n    // Copy data to device\n    CUDA_CHECK(cudaMemcpy(d_A, A.data(), M * K * sizeof(float), cudaMemcpyHostToDevice));\n    CUDA_CHECK(cudaMemcpy(d_B, B.data(), K * N * sizeof(float), cudaMemcpyHostToDevice));\n\n    // Set up kernel launch parameters\n    dim3 threadsPerBlock(16, 16);\n    dim3 blocksPerGrid((N + threadsPerBlock.x - 1) / threadsPerBlock.x, (M + threadsPerBlock.y - 1) / threadsPerBlock.y);\n\n    // Create CUDA events for timing\n    cudaEvent_t start, stop;\n    CUDA_CHECK(cudaEventCreate(&start));\n    CUDA_CHECK(cudaEventCreate(&stop));\n\n    // Launch kernel and measure time\n    CUDA_CHECK(cudaEventRecord(start));\n    matrixMulKernel<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, M, N, K);\n    CUDA_CHECK(cudaEventRecord(stop));\n    CUDA_CHECK(cudaEventSynchronize(stop));\n\n    float cuda_time_ms;\n    CUDA_CHECK(cudaEventElapsedTime(&cuda_time_ms, start, stop));\n\n    // Copy result back to host\n    CUDA_CHECK(cudaMemcpy(C.data(), d_C, M * N * sizeof(float), cudaMemcpyDeviceToHost));\n\n    // Sequential matrix multiplication for timing\n    double seq_start = omp_get_wtime();\n    sequentialMatrixMul(A, B, C_seq, M, N, K);\n    double seq_end = omp_get_wtime();\n    double seq_time = seq_end - seq_start;\n\n    // Output result matrix (trimmed if large)\n    cout << \"\\nResult Matrix C (first 5 elements, row-major):\\n\";\n    for (int i = 0; i < min(5, M * N); ++i) {\n        cout << C[i] << \" \";\n    }\n    if (M * N > 5) cout << \"...\";\n    cout << endl;\n\n    // Output execution times and stats\n    cout << \"\\nExecution Times:\\n\";\n    cout << \"CUDA Matrix Multiplication: \" << fixed << setprecision(6) << cuda_time_ms / 1000.0 << \" seconds\\n\";\n    cout << \"Sequential Matrix Multiplication: \" << seq_time << \" seconds\\n\";\n    cout << \"Speedup (Sequential / CUDA): \" << seq_time / (cuda_time_ms / 1000.0) << \"x\\n\";\n\n    // Free device memory\n    CUDA_CHECK(cudaFree(d_A));\n    CUDA_CHECK(cudaFree(d_B));\n    CUDA_CHECK(cudaFree(d_C));\n    CUDA_CHECK(cudaEventDestroy(start));\n    CUDA_CHECK(cudaEventDestroy(stop));\n\n    return 0;\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T16:43:52.437178Z","iopub.execute_input":"2025-05-06T16:43:52.437662Z","iopub.status.idle":"2025-05-06T16:43:52.447395Z","shell.execute_reply.started":"2025-05-06T16:43:52.437632Z","shell.execute_reply":"2025-05-06T16:43:52.446578Z"}},"outputs":[{"name":"stdout","text":"Overwriting matrix_mul.cu\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"!nvcc matrix_mul.cu -o matrix_mul -Xcompiler -fopenmp","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T16:43:52.801325Z","iopub.execute_input":"2025-05-06T16:43:52.801618Z","iopub.status.idle":"2025-05-06T16:43:56.111986Z","shell.execute_reply.started":"2025-05-06T16:43:52.801598Z","shell.execute_reply":"2025-05-06T16:43:56.110800Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"!./matrix_mul","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T16:44:04.841144Z","iopub.execute_input":"2025-05-06T16:44:04.841850Z","iopub.status.idle":"2025-05-06T16:44:21.762303Z","shell.execute_reply.started":"2025-05-06T16:44:04.841818Z","shell.execute_reply":"2025-05-06T16:44:21.761416Z"}},"outputs":[{"name":"stdout","text":"Using hardcoded matrix dimensions: A(1024x1024) * B(1024x1024) = C(1024x1024)\nGenerating random matrices...\n\nResult Matrix C (first 5 elements, row-major):\n26057.3 24626.1 25617.6 24769.9 24857.1 ...\n\nExecution Times:\nCUDA Matrix Multiplication: 0.024934 seconds\nSequential Matrix Multiplication: 16.486903 seconds\nSpeedup (Sequential / CUDA): 661.224716x\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}